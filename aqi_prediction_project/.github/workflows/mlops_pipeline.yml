name: MLOps Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'aqi_prediction_project/**'
  pull_request:
    branches: [ main ]
  schedule:
    # Run data collection every 6 hours
    - cron: '0 */6 * * *'
    # Run model retraining daily at 2 AM
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_type:
        description: 'Pipeline type'
        required: true
        default: 'full'
        type: choice
        options:
        - data_collection
        - model_training
        - full_pipeline
        - test_only

env:
  PYTHON_VERSION: '3.11'
  WORKING_DIRECTORY: './aqi_prediction_project'

jobs:
  test:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || github.event.inputs.run_type == 'test_only'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        cd ${{ env.WORKING_DIRECTORY }}
        pip install -r requirements.txt
        
    - name: Run linting
      run: |
        cd ${{ env.WORKING_DIRECTORY }}
        pip install flake8
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        
    - name: Run tests
      run: |
        cd ${{ env.WORKING_DIRECTORY }}
        python3 test_real_time_integration.py

  data_collection:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.run_type == 'data_collection' || github.event.inputs.run_type == 'full_pipeline'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        cd ${{ env.WORKING_DIRECTORY }}
        pip install -r requirements.txt
        
    - name: Collect real-time data
      run: |
        cd ${{ env.WORKING_DIRECTORY }}
        python3 -c "
        from real_time_data import RealTimeDataFetcher
        import json
        from datetime import datetime
        import os
        
        # Create data directory if it doesn't exist
        os.makedirs('data', exist_ok=True)
        
        fetcher = RealTimeDataFetcher()
        data = fetcher.get_current_data()
        
        if data:
            # Save data with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'data/real_time_data_{timestamp}.json'
            
            with open(filename, 'w') as f:
                json.dump(data, f, indent=2)
            
            print(f'‚úÖ Data collected and saved to {filename}')
            print(f'üìä Temperature: {data.get(\"temperature\", \"N/A\")}¬∞C')
            print(f'üíß Humidity: {data.get(\"humidity\", \"N/A\")}%')
            print(f'üå¨Ô∏è Wind Speed: {data.get(\"wind_speed\", \"N/A\")} m/s')
            print(f'üå´Ô∏è PM2.5: {data.get(\"pm25\", \"N/A\")} Œºg/m¬≥')
        else:
            print('‚ùå Failed to collect data')
            exit(1)
        "
        
    - name: Upload data artifacts
      uses: actions/upload-artifact@v3
      with:
        name: real-time-data-${{ github.run_number }}
        path: ${{ env.WORKING_DIRECTORY }}/data/real_time_data_*.json
        retention-days: 30

  model_training:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.run_type == 'model_training' || github.event.inputs.run_type == 'full_pipeline'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        cd ${{ env.WORKING_DIRECTORY }}
        pip install -r requirements.txt
        
    - name: Download historical data
      run: |
        cd ${{ env.WORKING_DIRECTORY }}
        # Check if historical data exists, if not collect it
        if [ ! -f "data/karachi_weather_1year.csv" ] || [ ! -f "data/karachi_air_quality_1year.csv" ]; then
          echo "üì• Collecting historical data..."
          python3 collect_yearly_data.py
        else
          echo "‚úÖ Historical data already exists"
        fi
        
    - name: Train and evaluate models
      run: |
        cd ${{ env.WORKING_DIRECTORY }}
        python3 -c "
        import pandas as pd
        import numpy as np
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.linear_model import Ridge
        from sklearn.preprocessing import StandardScaler
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        import joblib
        import os
        from datetime import datetime
        
        print('üöÄ Starting model training...')
        
        # Load data
        print('üìä Loading historical data...')
        weather_df = pd.read_csv('data/karachi_weather_1year.csv')
        air_quality_df = pd.read_csv('data/karachi_air_quality_1year.csv')
        
        # Merge datasets
        df = pd.merge(weather_df, air_quality_df, on='time', how='inner')
        print(f'üìà Dataset shape: {df.shape}')
        
        # Feature engineering
        print('üîß Engineering features...')
        df['time'] = pd.to_datetime(df['time'])
        df['hour'] = df['time'].dt.hour
        df['day'] = df['time'].dt.day
        df['month'] = df['time'].dt.month
        df['weekday'] = df['time'].dt.weekday
        df['is_weekend'] = (df['weekday'] >= 5).astype(int)
        
        # Cyclical encoding
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        
        # Interaction features
        df['temp_humidity_interaction'] = df['temperature_2m'] * df['relative_humidity_2m']
        df['wind_pollution_ratio'] = df['wind_speed_10m'] / (df['pm2_5'] + 1)
        
        # Select features
        feature_cols = [
            'temperature_2m', 'relative_humidity_2m', 'surface_pressure',
            'wind_speed_10m', 'wind_direction_10m', 'precipitation',
            'pm10', 'pm2_5', 'co', 'no2', 'o3', 'so2',
            'hour', 'day', 'month', 'weekday', 'is_weekend',
            'hour_sin', 'hour_cos', 'month_sin', 'month_cos',
            'temp_humidity_interaction', 'wind_pollution_ratio'
        ]
        
        X = df[feature_cols].fillna(0)
        y = df['pm2_5']  # Using PM2.5 as target
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Train models
        models = {
            'Ridge': Ridge(alpha=1.0),
            'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
        }
        
        results = {}
        for name, model in models.items():
            print(f'üéØ Training {name}...')
            model.fit(X_train_scaled, y_train)
            
            # Evaluate
            y_pred = model.predict(X_test_scaled)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            results[name] = {'rmse': rmse, 'mae': mae, 'r2': r2}
            print(f'üìä {name} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}')
        
        # Select best model
        best_model_name = min(results.keys(), key=lambda x: results[x]['rmse'])
        best_model = models[best_model_name]
        
        print(f'üèÜ Best model: {best_model_name}')
        print(f'üìà Best RMSE: {results[best_model_name][\"rmse\"]:.4f}')
        
        # Save model and scaler
        os.makedirs('models', exist_ok=True)
        joblib.dump(best_model, 'models/ridge_regression_best.pkl')
        joblib.dump(scaler, 'models/scaler.pkl')
        
        # Save results
        results_df = pd.DataFrame(results).T
        results_df.to_csv('models/model_comparison.csv')
        
        print('‚úÖ Model training completed successfully!')
        print(f'üíæ Model saved: models/ridge_regression_best.pkl')
        print(f'üìä Results saved: models/model_comparison.csv')
        "
        
    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      with:
        name: trained-model-${{ github.run_number }}
        path: ${{ env.WORKING_DIRECTORY }}/models/
        retention-days: 30

  api_test:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event.inputs.run_type == 'full_pipeline'
    needs: [model_training]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        cd ${{ env.WORKING_DIRECTORY }}
        pip install -r requirements.txt
        
    - name: Test API endpoints
      run: |
        cd ${{ env.WORKING_DIRECTORY }}
        python3 -c "
        from api import app
        import threading
        import time
        import requests
        import json
        
        print('üöÄ Starting API tests...')
        
        # Start API in background
        def run_api():
            app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)
        
        api_thread = threading.Thread(target=run_api)
        api_thread.daemon = True
        api_thread.start()
        
        # Wait for API to start
        print('‚è≥ Waiting for API to start...')
        time.sleep(10)
        
        # Test endpoints
        base_url = 'http://localhost:5000'
        
        try:
            # Test health endpoint
            print('üîç Testing health endpoint...')
            response = requests.get(f'{base_url}/health', timeout=15)
            if response.status_code == 200:
                print('‚úÖ Health check passed')
                print(f'üìä Response: {response.json()}')
            else:
                print(f'‚ùå Health check failed: {response.status_code}')
                exit(1)
                
            # Test single prediction
            print('üîç Testing single prediction...')
            response = requests.get(f'{base_url}/predict/single', timeout=15)
            if response.status_code == 200:
                data = response.json()
                print(f'‚úÖ Single prediction passed: AQI = {data.get(\"aqi\", \"N/A\")}')
            else:
                print(f'‚ùå Single prediction failed: {response.status_code}')
                exit(1)
                
            # Test 3-day prediction
            print('üîç Testing 3-day prediction...')
            response = requests.get(f'{base_url}/predict', timeout=15)
            if response.status_code == 200:
                data = response.json()
                print(f'‚úÖ 3-day prediction passed')
                print(f'üìä Day 1 AQI: {data.get(\"day1_aqi\", \"N/A\")}')
                print(f'üìä Day 2 AQI: {data.get(\"day2_aqi\", \"N/A\")}')
                print(f'üìä Day 3 AQI: {data.get(\"day3_aqi\", \"N/A\")}')
            else:
                print(f'‚ùå 3-day prediction failed: {response.status_code}')
                exit(1)
                
            print('üéâ All API tests passed!')
            
        except Exception as e:
            print(f'‚ùå API test failed: {e}')
            exit(1)
        "

  notification:
    runs-on: ubuntu-latest
    if: always()
    needs: [test, data_collection, model_training, api_test]
    
    steps:
    - name: Notify pipeline status
      run: |
        echo "üìä Pipeline Status Summary:"
        echo "=========================="
        echo "Test: ${{ needs.test.result }}"
        echo "Data Collection: ${{ needs.data_collection.result }}"
        echo "Model Training: ${{ needs.model_training.result }}"
        echo "API Test: ${{ needs.api_test.result }}"
        echo "=========================="
        
        if [ "${{ needs.test.result }}" == "success" ] && [ "${{ needs.data_collection.result }}" == "success" ] && [ "${{ needs.model_training.result }}" == "success" ] && [ "${{ needs.api_test.result }}" == "success" ]; then
          echo "üéâ All pipeline steps completed successfully!"
        else
          echo "‚ö†Ô∏è Some pipeline steps failed. Check the logs for details."
        fi